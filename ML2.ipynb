{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8Vw59_NJaxdB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import os.path as path\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
      ],
      "metadata": {
        "id": "mWIIQPpJbIpt"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from lightgbm import LGBMClassifier\n"
      ],
      "metadata": {
        "id": "PfjbxRk7bIrs"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "gej0k7PHbIvE"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = train[\"y\"]\n",
        "X = train.drop([\"y\", \"id\"], axis=1)"
      ],
      "metadata": {
        "id": "Jq9UOndVbRkD"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ids = test[\"id\"]\n",
        "X_test = test.drop(\"id\", axis=1)"
      ],
      "metadata": {
        "id": "EtciiDxabRni"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Define Feature Types\n",
        "\n",
        "non_bin_features = [\"job\", \"education\", \"contact\", \"month\", \"poutcome\", \"marital\"]\n",
        "bin_features = [\"default\", \"housing\", \"loan\"]\n",
        "cat_features = bin_features + non_bin_features\n",
        "num_features = [col for col in X.columns if col not in cat_features]\n",
        "\n",
        "# Preprocessing pipeline\n",
        "column_transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_features),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "pN9tTXIKbYJ7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LightGBM\n",
        "\n",
        "# ============================\n",
        "# 3. Train/Validation Split\n",
        "# ============================\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# 4. Define LightGBM + Hyperparameter Grid\n",
        "# ============================\n",
        "lgb_base = LGBMClassifier(\n",
        "    objective=\"binary\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "param_dist = {\n",
        "    \"classifier__n_estimators\": [300, 500, 700],\n",
        "    \"classifier__learning_rate\": [0.01, 0.05, 0.1],\n",
        "    \"classifier__max_depth\": [-1, 6, 10],   # -1 means no limit\n",
        "    \"classifier__num_leaves\": [31, 64, 128],\n",
        "    \"classifier__min_child_samples\": [20, 50, 100],\n",
        "    \"classifier__subsample\": [0.7, 0.8, 0.9, 1.0],\n",
        "    \"classifier__colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
        "    \"classifier__reg_alpha\": [0, 0.1, 0.5],\n",
        "    \"classifier__reg_lambda\": [0.5, 1.0, 2.0]\n",
        "}\n",
        "\n",
        "# ============================\n",
        "# 5. Create Pipeline\n",
        "# ============================\n",
        "pipeline = Pipeline(steps=[\n",
        "    (\"preprocessor\", column_transformer),\n",
        "    (\"classifier\", lgb_base)\n",
        "])\n",
        "\n",
        "# ============================\n",
        "# 6. RandomizedSearchCV\n",
        "# ============================\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit on training set\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best hyperparameters:\", random_search.best_params_)\n",
        "print(\"Validation ROC-AUC:\", random_search.best_score_)\n",
        "\n",
        "# ============================\n",
        "# 7. Evaluate on Validation Set\n",
        "# ============================\n",
        "best_model = random_search.best_estimator_\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "y_val_prob = best_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "val_auc = roc_auc_score(y_val, y_val_prob)\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}, ROC-AUC: {val_auc:.4f}\")\n",
        "\n",
        "# ============================\n",
        "# 8. Predict on Test Set and Save Submission\n",
        "# ============================\n",
        "y_test_probs = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "submission = pd.DataFrame({\"id\": test_ids, \"y\": y_test_probs})\n",
        "submission.to_csv(\"submission4.csv\", index=False)\n",
        "print(\"✅ Submission file saved as submission4.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "Jiw6K-AhTH8t",
        "outputId": "de05228d-2d91-45e0-f1e0-dc7511a122c8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2825765627.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Fit on training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best hyperparameters:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1952\u001b[0m             ParameterSampler(\n\u001b[1;32m   1953\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "# from lightgbm import LGBMClassifier\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from scipy.stats import uniform, randint\n",
        "# import numpy as np\n",
        "\n",
        "# # ===========================\n",
        "# # Preprocessing (same as before)\n",
        "# # ===========================\n",
        "# preprocessor = ColumnTransformer(\n",
        "#     transformers=[\n",
        "#         (\"num\", StandardScaler(), num_features),\n",
        "#         (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_features)\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# # Base LightGBM model\n",
        "# lgbm = LGBMClassifier(\n",
        "#     objective=\"binary\",\n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# pipe = Pipeline(steps=[(\"preprocessor\", preprocessor),\n",
        "#                        (\"classifier\", lgbm)])\n",
        "\n",
        "# # ===========================\n",
        "# # Refined Search Space\n",
        "# # (around your best params)\n",
        "# # ===========================\n",
        "# param_dist = {\n",
        "#     \"classifier__num_leaves\": randint(50, 120),       # Best was 60\n",
        "#     \"classifier__max_depth\": randint(3, 8),           # Best was 4\n",
        "#     \"classifier__n_estimators\": randint(600, 1200),   # Best was 800\n",
        "#     \"classifier__learning_rate\": uniform(0.05, 0.15), # Center around 0.1\n",
        "#     \"classifier__subsample\": uniform(0.5, 0.3),       # Best was 0.6\n",
        "#     \"classifier__colsample_bytree\": uniform(0.7, 0.3),# Best was 1.0\n",
        "#     \"classifier__reg_alpha\": randint(0, 3),           # Best was 1\n",
        "#     \"classifier__reg_lambda\": randint(0, 5),          # Best was 0\n",
        "#     \"classifier__min_child_samples\": randint(20, 100) # New: controls leaf overfitting\n",
        "# }\n",
        "\n",
        "# # ===========================\n",
        "# # Randomized Search CV\n",
        "# # ===========================\n",
        "# random_search_refined = RandomizedSearchCV(\n",
        "#     pipe,\n",
        "#     param_distributions=param_dist,\n",
        "#     n_iter=50,              # More focused iterations\n",
        "#     scoring=\"roc_auc\",\n",
        "#     cv=3,\n",
        "#     verbose=2,\n",
        "#     n_jobs=-1,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# # Fit\n",
        "# random_search_refined.fit(X_train, y_train)\n",
        "\n",
        "# # Best params and score\n",
        "# print(\"Best params from Refined RandomizedSearchCV:\")\n",
        "# print(random_search_refined.best_params_)\n",
        "# print(\"Best ROC-AUC:\", random_search_refined.best_score_)\n",
        "\n",
        "# # Evaluate on holdout\n",
        "# best_model = random_search_refined.best_estimator_\n",
        "# y_pred = best_model.predict(X_test)\n",
        "# y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "# print(\"Test ROC-AUC:\", roc_auc_score(y_test, y_pred_proba))\n"
      ],
      "metadata": {
        "id": "7XAza_VrF7zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 3. Train/Validation Split\n",
        "# # ============================\n",
        "# X_train, X_val, y_train, y_val = train_test_split(\n",
        "#     X, y, test_size=0.2, stratify=y, random_state=42\n",
        "# )\n",
        "\n",
        "# # ============================\n",
        "# # 4. Define Base Models + Param Grids\n",
        "# # ============================\n",
        "\n",
        "# # LightGBM\n",
        "# lgb_base = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
        "# lgb_param_dist = {\n",
        "#     \"classifier__n_estimators\": [300, 500, 700],\n",
        "#     \"classifier__learning_rate\": [0.01, 0.05, 0.1],\n",
        "#     \"classifier__max_depth\": [-1, 6, 10],\n",
        "#     \"classifier__num_leaves\": [31, 64, 128],\n",
        "#     \"classifier__min_child_samples\": [20, 50, 100],\n",
        "#     \"classifier__subsample\": [0.7, 0.8, 0.9, 1.0],\n",
        "#     \"classifier__colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
        "#     \"classifier__reg_alpha\": [0, 0.1, 0.5],\n",
        "#     \"classifier__reg_lambda\": [0.5, 1.0, 2.0]\n",
        "# }\n",
        "\n",
        "# # XGBoost\n",
        "# xgb_base = XGBClassifier(\n",
        "#     objective=\"binary:logistic\",\n",
        "#     eval_metric=\"auc\",\n",
        "#     use_label_encoder=False,\n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "# xgb_param_dist = {\n",
        "#     \"classifier__n_estimators\": [300, 500, 600],\n",
        "#     \"classifier__learning_rate\": [0.01, 0.05, 0.1],\n",
        "#     \"classifier__max_depth\": [4, 6, 8],\n",
        "#     \"classifier__min_child_weight\": [1, 3, 5],\n",
        "#     \"classifier__gamma\": [0, 0.1, 0.3],\n",
        "#     \"classifier__subsample\": [0.7, 0.8, 0.9],\n",
        "#     \"classifier__colsample_bytree\": [0.7, 0.8, 0.9],\n",
        "#     \"classifier__reg_alpha\": [0, 0.1, 0.5],\n",
        "#     \"classifier__reg_lambda\": [0.5, 1.0, 2.0]\n",
        "# }\n",
        "\n",
        "# # ============================\n",
        "# # 5. RandomizedSearchCV for LightGBM\n",
        "# # ============================\n",
        "# lgb_pipeline = Pipeline([\n",
        "#     (\"preprocessor\", column_transformer),\n",
        "#     (\"classifier\", lgb_base)\n",
        "# ])\n",
        "\n",
        "# lgb_search = RandomizedSearchCV(\n",
        "#     estimator=lgb_pipeline,\n",
        "#     param_distributions=lgb_param_dist,\n",
        "#     n_iter=20,\n",
        "#     scoring=\"roc_auc\",\n",
        "#     cv=3,\n",
        "#     verbose=2,\n",
        "#     n_jobs=-1,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# lgb_search.fit(X_train, y_train)\n",
        "# print(\"Best LightGBM params:\", lgb_search.best_params_)\n",
        "# print(\"Best LightGBM ROC-AUC:\", lgb_search.best_score_)\n",
        "\n",
        "# # ============================\n",
        "# # 6. RandomizedSearchCV for XGBoost\n",
        "# # ============================\n",
        "# xgb_pipeline = Pipeline([\n",
        "#     (\"preprocessor\", column_transformer),\n",
        "#     (\"classifier\", xgb_base)\n",
        "# ])\n",
        "\n",
        "# xgb_search = RandomizedSearchCV(\n",
        "#     estimator=xgb_pipeline,\n",
        "#     param_distributions=xgb_param_dist,\n",
        "#     n_iter=20,\n",
        "#     scoring=\"roc_auc\",\n",
        "#     cv=3,\n",
        "#     verbose=2,\n",
        "#     n_jobs=-1,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# xgb_search.fit(X_train, y_train)\n",
        "# print(\"Best XGBoost params:\", xgb_search.best_params_)\n",
        "# print(\"Best XGBoost ROC-AUC:\", xgb_search.best_score_)\n",
        "\n",
        "# # ============================\n",
        "# # 7. Ensemble Voting\n",
        "# # ============================\n",
        "# ensemble = VotingClassifier(\n",
        "#     estimators=[\n",
        "#         (\"lgbm\", lgb_search.best_estimator_),\n",
        "#         (\"xgb\", xgb_search.best_estimator_)\n",
        "#     ],\n",
        "#     voting=\"soft\"  # average predicted probabilities\n",
        "# )\n",
        "\n",
        "# ensemble.fit(X_train, y_train)\n",
        "\n",
        "# # Validation\n",
        "# y_val_pred = ensemble.predict(X_val)\n",
        "# y_val_prob = ensemble.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# val_acc = accuracy_score(y_val, y_val_pred)\n",
        "# val_auc = roc_auc_score(y_val, y_val_prob)\n",
        "# print(f\"Ensemble Validation Accuracy: {val_acc:.4f}, ROC-AUC: {val_auc:.4f}\")\n",
        "\n",
        "# # ============================\n",
        "# # 8. Predict Test Set\n",
        "# # ============================\n",
        "# y_test_probs = ensemble.predict_proba(X_test)[:, 1]\n",
        "# submission = pd.DataFrame({\"id\": test_ids, \"y\": y_test_probs})\n",
        "# submission.to_csv(\"submission_ensemble_randomsearch.csv\", index=False)\n",
        "# print(\"✅ Ensemble submission saved as submission_ensemble_randomsearch.csv\")"
      ],
      "metadata": {
        "id": "ng0mT17ldEOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # ============================\n",
        "# # 3. Train/Validation Split\n",
        "# # ============================\n",
        "# X_train, X_val, y_train, y_val = train_test_split(\n",
        "#     X, y, test_size=0.2, stratify=y, random_state=42\n",
        "# )\n",
        "\n",
        "# # ============================\n",
        "# # 4. Define Base Models\n",
        "# # ============================\n",
        "# lgb_base = LGBMClassifier(\n",
        "#     objective=\"binary\",\n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# xgb_base = XGBClassifier(\n",
        "#     objective=\"binary:logistic\",\n",
        "#     eval_metric=\"logloss\",\n",
        "#     use_label_encoder=False,\n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# # ============================\n",
        "# # 5. Create Pipelines\n",
        "# # ============================\n",
        "# lgb_pipeline = Pipeline(steps=[\n",
        "#     (\"preprocessor\", column_transformer),\n",
        "#     (\"classifier\", lgb_base)\n",
        "# ])\n",
        "\n",
        "# xgb_pipeline = Pipeline(steps=[\n",
        "#     (\"preprocessor\", column_transformer),\n",
        "#     (\"classifier\", xgb_base)\n",
        "# ])\n",
        "\n",
        "# # ============================\n",
        "# # 6. Ensemble with Voting\n",
        "# # ============================\n",
        "# ensemble = VotingClassifier(\n",
        "#     estimators=[\n",
        "#         (\"lgbm\", lgb_pipeline),\n",
        "#         (\"xgb\", xgb_pipeline)\n",
        "#     ],\n",
        "#     voting=\"soft\"   # average probabilities\n",
        "# )\n",
        "\n",
        "# # ============================\n",
        "# # 7. Train Ensemble\n",
        "# # ============================\n",
        "# ensemble.fit(X_train, y_train)\n",
        "\n",
        "# # Validation Performance\n",
        "# y_val_pred = ensemble.predict(X_val)\n",
        "# y_val_prob = ensemble.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# val_acc = accuracy_score(y_val, y_val_pred)\n",
        "# val_auc = roc_auc_score(y_val, y_val_prob)\n",
        "# print(f\"Validation Accuracy: {val_acc:.4f}, ROC-AUC: {val_auc:.4f}\")\n",
        "\n",
        "# # ============================\n",
        "# # 8. Predict on Test Set\n",
        "# # ============================\n",
        "# y_test_probs = ensemble.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# submission = pd.DataFrame({\"id\": test_ids, \"y\": y_test_probs})\n",
        "# submission.to_csv(\"submission_ensemble.csv\", index=False)\n",
        "# print(\" Ensemble submission saved as submission_ensemble.csv\")"
      ],
      "metadata": {
        "id": "r6lgtHfQbDb0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}